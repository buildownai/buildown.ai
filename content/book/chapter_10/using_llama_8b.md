---
title: Evaluating Llama 3.1 8B
description: This chapter explores the evaluation of the Llama 3.1 8B model with different temperature settings to assess its performance in generating question-answer pairs and handling queries.
keywords:
- Llama 3.1 8B
- Model Evaluation
- Temperature Settings
- Data Generation
- Query Results
---

In this chapter, you'll dive into the fascinating world of evaluating a custom-built AI model based on **Llama 3.1 8B** with an enhanced context size of 32k. Weâ€™ll explore how adjusting the temperature parameter influences the model's performance and output quality.

You will learn about:

- Creating your own custom model using Docker.
- Setting up different temperature levels to observe their impact on data generation and query results.
- Analyzing the effectiveness of question-answer pair creation under various conditions.
- Understanding the nuances between chunk, QA, and graph versions of responses.

By the end of this chapter, you'll have a clearer understanding of how tweaking parameters can significantly affect your AI model's behavior and output quality. Dive in to see how subtle changes can lead to vastly different results!