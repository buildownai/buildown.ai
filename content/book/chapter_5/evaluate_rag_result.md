---
title: Evaluate RAG based Answers
description: Learn how to evaluate and refine responses generated by Retrieval-Augmented Generation (RAG) models for accuracy and relevance.
keywords:
  - RAG
  - evaluation
  - debugging
  - AI chatbot
  - PURISTA
---

In this chapter, you'll dive into the crucial process of evaluating and refining answers produced by your AI system. We’ll explore how to break down a model's response into smaller components for better understanding and improvement.

You will start by rating the answer based on its correctness and relevance using a structured table format. This exercise helps identify areas where the information is accurate but also highlights irrelevant or missing details, guiding you towards clearer expectations from your AI system.

Next, we’ll define precise expectations for the output of your AI, deciding whether to aim for short and concise answers or more detailed ones that might include examples or installation instructions. By breaking down these expectations into specific items, you can pinpoint where improvements are needed in your system prompt.

Improving the system prompt is a key step in enhancing RAG-based responses. You’ll learn how to adjust prompts to better utilize context and align with your defined expectations, ensuring answers remain relevant and accurate.

Throughout this chapter, we'll also cover practical tips for testing and refining your AI's performance using fixed test queries, helping you ensure consistency and correctness across various scenarios. Dive in to see how you can elevate the quality of your AI’s responses!