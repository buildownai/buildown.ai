{"parsed":{"_path":"/book/chapter_10/using_llama_3b","_dir":"chapter_10","_draft":false,"_partial":false,"_locale":"","title":"Evaluating Llama 3.2 3B","description":"Explore the evaluation of the custom model `buildownai/llama3b` based on **Llama 3.2 3B** with a focus on its performance at different temperatures.","keywords":["Llama 3.2 3B","Custom Model Evaluation","Temperature Settings","Data Generation","Query Results"],"body":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"In this chapter, you'll dive into the evaluation of our custom AI model "},{"type":"element","tag":"code","props":{"className":[]},"children":[{"type":"text","value":"buildownai/llama3b"}]},{"type":"text","value":", which is based on "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Llama 3.2 3B"}]},{"type":"text","value":" and has a large context size for better performance during inference. We will explore how adjusting the temperature parameter affects the model's output quality and reliability."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"You'll learn about setting up the model with specific configurations, such as increasing its attention span to handle more context. Then, weâ€™ll evaluate the model at two different temperatures: "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"0.01"}]},{"type":"text","value":" and "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"1"}]},{"type":"text","value":", observing how these settings impact data generation and query results."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Discover insights into the model's performance in generating precise answers and handling complex queries related to a TypeScript backend framework called PURISTA. We'll also discuss common issues like hallucinations, broken links, and context differentiation challenges that arise at higher temperatures."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Join us as we uncover the nuances of fine-tuning AI models for optimal usability and reliability!"}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[]}},"_type":"markdown","_id":"content:book:chapter_10:using_llama_3b.md","_source":"content","_file":"book/chapter_10/using_llama_3b.md","_stem":"book/chapter_10/using_llama_3b","_extension":"md","sitemap":{"loc":"/book/chapter_10/using_llama_3b"}},"hash":"MMnktvOr6l"}